{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34de1b0",
   "metadata": {},
   "source": [
    "# Saying Hello to MNIST Dataset\n",
    "\n",
    "'Hello MNIST Dataset!ðŸ‘‹'\n",
    "\n",
    "In this notebook we are going to take a look at the MNIST dataset and build a simple model, as well as testing it by using TensorFlow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106578e",
   "metadata": {},
   "source": [
    "### Importing relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caded792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46d2fe7",
   "metadata": {},
   "source": [
    "### Loading the dataset from TFDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d72ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 17:12:50.455509: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-21 17:12:50.455729: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8eaa951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    version=1.0.0,\n",
       "    description='The MNIST database of handwritten digits.',\n",
       "    urls=['https://storage.googleapis.com/cvdf-datasets/mnist/'],\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    total_num_examples=70000,\n",
       "    splits={\n",
       "        'test': 10000,\n",
       "        'train': 60000,\n",
       "    },\n",
       "    supervised_keys=('image', 'label'),\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4d332",
   "metadata": {},
   "source": [
    "### Scaling and shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1467efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A validation data is not provided by TFDS\n",
    "# To split the ds_train to test & validation we need the sample size,\n",
    "# so we can skip/take the data into ds_test & ds_valid\n",
    "num_train_samples = ds_info.splits['train'].num_examples\n",
    "num_train_samples = tf.cast(num_train_samples, tf.int64)\n",
    "\n",
    "num_valid_samples = 0.1 * ds_info.splits['train'].num_examples\n",
    "num_valid_samples = tf.cast(num_valid_samples, tf.int64)\n",
    "\n",
    "num_test_samples = ds_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd105340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to normalize the data because TFDS provide images of type tf.uint8, while the model expects tf.float32\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # Since there are 256 different shades of grey, we divide the pixel values by 255 to get the desired results.\n",
    "    # All elements are going to be between 0 and 1.\n",
    "    image /= 255.\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27adc959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling and shuffling\n",
    "BUFFER_SIZE = 10000\n",
    "ds_train_and_valid = ds_train.map(scale)\n",
    "ds_train_and_valid = ds_train_and_valid.shuffle(buffer_size=BUFFER_SIZE)\n",
    "# uniformly shuffling casue buffer_size = number of samples\n",
    "\n",
    "# Assigning validation and train data\n",
    "ds_valid = ds_train_and_valid.take(num_valid_samples)\n",
    "ds_train = ds_train_and_valid.skip(num_valid_samples)\n",
    "\n",
    "# Batching\n",
    "BATCH_SIZE = 100\n",
    "ds_train = ds_train.batch(BATCH_SIZE)\n",
    "ds_valid = ds_valid.batch(num_valid_samples)\n",
    "\n",
    "# Scaling and batching the test data aswell\n",
    "ds_test = ds_test.map(scale)\n",
    "ds_test = ds_test.batch(BATCH_SIZE)\n",
    "\n",
    "# Taking the only batch\n",
    "validation_inputs, validation_targets = next(iter(ds_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4e9bfc",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "451d4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 # We had (28,28,1) tensors because the images have 28x28 pixel ratio -> 28x28 = 784 input size.\n",
    "output_size = 10 # For each digit, 0 to 9, we have one output node -> 10.\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    \n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "# Choosing the optimizer and the loss function\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0562949",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04e5c79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 17:14:02.997763: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540/540 - 60s - loss: 1.6305 - sparse_categorical_accuracy: 0.8594 - val_loss: 0.0000e+00 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/6\n",
      "540/540 - 60s - loss: 1.5344 - sparse_categorical_accuracy: 0.9343 - val_loss: 1.5232 - val_sparse_categorical_accuracy: 0.9430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 17:15:03.426729: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/6\n",
      "540/540 - 58s - loss: 1.5201 - sparse_categorical_accuracy: 0.9458 - val_loss: 1.5178 - val_sparse_categorical_accuracy: 0.9480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 17:16:01.462847: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/6\n",
      "540/540 - 61s - loss: 1.5130 - sparse_categorical_accuracy: 0.9521 - val_loss: 1.5110 - val_sparse_categorical_accuracy: 0.9540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 17:17:02.245350: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/6\n",
      "540/540 - 60s - loss: 1.5071 - sparse_categorical_accuracy: 0.9575 - val_loss: 1.5076 - val_sparse_categorical_accuracy: 0.9568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 17:18:01.878229: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/6\n",
      "540/540 - 60s - loss: 1.5023 - sparse_categorical_accuracy: 0.9615 - val_loss: 1.5041 - val_sparse_categorical_accuracy: 0.9597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 17:19:02.155529: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x10f74f4d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 6\n",
    "\n",
    "model.fit(ds_train, \n",
    "    epochs=NUM_EPOCHS, \n",
    "    validation_data=(validation_inputs, validation_targets), \n",
    "    verbose =2,\n",
    "    validation_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fce7c9",
   "metadata": {},
   "source": [
    "For the validation data set we have 96% accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba210d50",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d719b0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 15s 154ms/step - loss: 1.5073 - sparse_categorical_accuracy: 0.9554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 17:22:21.970728: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51ac0e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.51. Test accuracy: 95.54%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f02ce1",
   "metadata": {},
   "source": [
    "Unfortunately for the test data we got 95% accuracy this time. To improve this we can try to increase our hidden layer size, add more hidden layers or tinker around with the parameters.\n",
    "\n",
    "Note that all of these methods **should be applied before** we test our model, i.e. in training step. Otherwise we will be overfitting to the test data, which defeats the whole purpose. Also we should be careful not to overfit to the validation data. In this example we can see that our model lost around 0.5% accuracy which is pretty neat nonetheless, considering the best results for MNIST dataset tops around 98% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
